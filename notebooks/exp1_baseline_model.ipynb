{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Using cached mlflow-2.15.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting mlflow-skinny==2.15.1 (from mlflow)\n",
      "  Using cached mlflow_skinny-2.15.1-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: Flask<4 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (3.0.3)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
      "  Using cached alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Using cached docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Using cached graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (3.4.1)\n",
      "Requirement already satisfied: matplotlib<4 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (3.8.4)\n",
      "Requirement already satisfied: numpy<2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (1.26.4)\n",
      "Requirement already satisfied: pandas<3 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (2.2.2)\n",
      "Requirement already satisfied: pyarrow<16,>=4.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (14.0.2)\n",
      "Collecting querystring-parser<2 (from mlflow)\n",
      "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\n",
      "Requirement already satisfied: scikit-learn<2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (1.4.2)\n",
      "Requirement already satisfied: scipy<2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (1.13.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (2.0.30)\n",
      "Requirement already satisfied: Jinja2<4,>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow) (3.1.4)\n",
      "Collecting waitress<4 (from mlflow)\n",
      "  Using cached waitress-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.15.1->mlflow) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.15.1->mlflow) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.15.1->mlflow) (2.2.1)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.15.1->mlflow)\n",
      "  Using cached databricks_sdk-0.30.0-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: entrypoints<1 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.15.1->mlflow) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.15.1->mlflow) (3.1.37)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.15.1->mlflow) (7.0.1)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.15.1->mlflow)\n",
      "  Using cached opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.15.1->mlflow)\n",
      "  Using cached opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging<25 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from mlflow-skinny==2.15.1->mlflow) (24.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.15.1->mlflow) (3.20.3)\n",
      "Requirement already satisfied: pytz<2025 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.15.1->mlflow) (2024.1)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.15.1->mlflow) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.15.1->mlflow) (2.32.2)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.15.1->mlflow)\n",
      "  Using cached sqlparse-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
      "  Using cached Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\programdata\\anaconda3\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (4.11.0)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from docker<8,>=4.0.0->mlflow) (306)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (2.2.2)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from Flask<4->mlflow) (3.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from Flask<4->mlflow) (1.6.2)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Using cached graphql_core-3.2.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Using cached graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting aniso8601<10,>=8 (from graphene<4->mlflow)\n",
      "  Using cached aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from Jinja2<4,>=3.0->mlflow) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<3->mlflow) (2023.3)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from querystring-parser<2->mlflow) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn<2->mlflow) (2.2.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from click<9,>=7.0->mlflow-skinny==2.15.1->mlflow) (0.4.6)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.1->mlflow)\n",
      "  Using cached google_auth-2.33.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.15.1->mlflow) (4.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow-skinny==2.15.1->mlflow) (3.17.0)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.15.1->mlflow)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.15.1->mlflow)\n",
      "  Using cached opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.15.1->mlflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.15.1->mlflow) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.15.1->mlflow) (2024.7.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.15.1->mlflow) (1.14.1)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.15.1->mlflow) (4.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.1->mlflow) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.1->mlflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.1->mlflow) (0.4.8)\n",
      "Using cached mlflow-2.15.1-py3-none-any.whl (26.3 MB)\n",
      "Using cached mlflow_skinny-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "Using cached alembic-1.13.2-py3-none-any.whl (232 kB)\n",
      "Using cached docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Using cached graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
      "Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Using cached waitress-3.0.0-py3-none-any.whl (56 kB)\n",
      "Using cached aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "Using cached databricks_sdk-0.30.0-py3-none-any.whl (538 kB)\n",
      "Using cached graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
      "Using cached graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n",
      "Using cached opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n",
      "Using cached sqlparse-0.5.1-py3-none-any.whl (44 kB)\n",
      "Using cached Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached google_auth-2.33.0-py2.py3-none-any.whl (200 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: aniso8601, waitress, sqlparse, rsa, querystring-parser, Mako, graphql-core, deprecated, opentelemetry-api, graphql-relay, google-auth, docker, alembic, opentelemetry-semantic-conventions, graphene, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow\n",
      "Successfully installed Mako-1.3.5 alembic-1.13.2 aniso8601-9.0.1 databricks-sdk-0.30.0 deprecated-1.2.14 docker-7.1.0 google-auth-2.33.0 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 mlflow-2.15.1 mlflow-skinny-2.15.1 opentelemetry-api-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 querystring-parser-1.2.4 rsa-4.9 sqlparse-0.5.1 waitress-3.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import mlflow.sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2     sadness                Funeral ceremony...gloomy friday...\n",
       "3  enthusiasm               wants to hang out with friends SOON!\n",
       "4     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/campusx-official/jupyter-masterclass/main/tweet_emotions.csv').drop(columns=['tweet_id'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:32: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:32: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_7376\\850274854.py:32: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text = re.sub('\\s+', ' ', text).strip()\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "\n",
    "# Define text preprocessing functions\n",
    "def lemmatization(text):\n",
    "    \"\"\"Lemmatize the text.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    \"\"\"Remove stop words from the text.\"\"\"\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = [word for word in str(text).split() if word not in stop_words]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def removing_numbers(text):\n",
    "    \"\"\"Remove numbers from the text.\"\"\"\n",
    "    text = ''.join([char for char in text if not char.isdigit()])\n",
    "    return text\n",
    "\n",
    "def lower_case(text):\n",
    "    \"\"\"Convert text to lower case.\"\"\"\n",
    "    text = text.split()\n",
    "    text = [word.lower() for word in text]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def removing_punctuations(text):\n",
    "    \"\"\"Remove punctuations from the text.\"\"\"\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = text.replace('؛', \"\")\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def removing_urls(text):\n",
    "    \"\"\"Remove URLs from the text.\"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def normalize_text(df):\n",
    "    \"\"\"Normalize the text data.\"\"\"\n",
    "    try:\n",
    "        df['content'] = df['content'].apply(lower_case)\n",
    "        df['content'] = df['content'].apply(remove_stop_words)\n",
    "        df['content'] = df['content'].apply(removing_numbers)\n",
    "        df['content'] = df['content'].apply(removing_punctuations)\n",
    "        df['content'] = df['content'].apply(removing_urls)\n",
    "        df['content'] = df['content'].apply(lemmatization)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f'Error during text normalization: {e}')\n",
    "        raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during text normalization: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mstopwords\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('stopwords')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\hp/nltk_data'\n",
      "    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n",
      "    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_7376\\850274854.py:32: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text = re.sub('\\s+', ' ', text).strip()\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\hp/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\hp/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[1;32mIn[5], line 44\u001b[0m, in \u001b[0;36mnormalize_text\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(lower_case)\n\u001b[1;32m---> 44\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_stop_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(removing_numbers)\n\u001b[0;32m     46\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(removing_punctuations)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m, in \u001b[0;36mremove_stop_words\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_stop_words\u001b[39m(text):\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Remove stop words from the text.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     14\u001b[0m     text \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(text)\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\hp/nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "df = normalize_text(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>tiffanylue know listenin bad habit earlier sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed headache ughhhh waitin call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>want hang friend soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>dannycastillo want trade someone houston ticke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  tiffanylue know listenin bad habit earlier sta...\n",
       "1     sadness            layin n bed headache ughhhh waitin call\n",
       "2     sadness                     funeral ceremony gloomy friday\n",
       "3  enthusiasm                              want hang friend soon\n",
       "4     neutral  dannycastillo want trade someone houston ticke..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = normalize_text(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "neutral       8638\n",
       "worry         8459\n",
       "happiness     5209\n",
       "sadness       5165\n",
       "love          3842\n",
       "surprise      2187\n",
       "fun           1776\n",
       "relief        1526\n",
       "hate          1323\n",
       "empty          827\n",
       "enthusiasm     759\n",
       "boredom        179\n",
       "anger          110\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['sentiment'].isin(['happiness','sadness'])\n",
    "df = df[x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_7376\\3057201912.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['sentiment'] = df['sentiment'].replace({'sadness':0, 'happiness':1})\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_7376\\3057201912.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment'] = df['sentiment'].replace({'sadness':0, 'happiness':1})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>layin n bed headache ughhhh waitin call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>sleep im thinking old friend want married damn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>charviray charlene love miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>kelcouch sorry least friday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            content\n",
       "1          0            layin n bed headache ughhhh waitin call\n",
       "2          0                     funeral ceremony gloomy friday\n",
       "6          0  sleep im thinking old friend want married damn...\n",
       "8          0                       charviray charlene love miss\n",
       "9          0                        kelcouch sorry least friday"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'] = df['sentiment'].replace({'sadness':0, 'happiness':1})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(df['content'])\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dagshub\n",
      "  Using cached dagshub-0.3.34-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: PyYAML>=5 in c:\\programdata\\anaconda3\\lib\\site-packages (from dagshub) (6.0.1)\n",
      "Collecting fusepy>=3 (from dagshub)\n",
      "  Using cached fusepy-3.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from dagshub) (1.4.4)\n",
      "Requirement already satisfied: click>=8.0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from dagshub) (8.1.7)\n",
      "Collecting httpx~=0.23.0 (from dagshub)\n",
      "  Using cached httpx-0.23.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: GitPython>=3.1.29 in c:\\programdata\\anaconda3\\lib\\site-packages (from dagshub) (3.1.37)\n",
      "Collecting rich~=13.1.0 (from dagshub)\n",
      "  Using cached rich-13.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting dacite~=1.6.0 (from dagshub)\n",
      "  Using cached dacite-1.6.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: tenacity~=8.2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from dagshub) (8.2.2)\n",
      "Collecting gql[requests] (from dagshub)\n",
      "  Using cached gql-3.5.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting dataclasses-json (from dagshub)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from dagshub) (2.2.2)\n",
      "Collecting treelib~=1.6.4 (from dagshub)\n",
      "  Using cached treelib-1.6.4-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting pathvalidate~=3.0.0 (from dagshub)\n",
      "  Using cached pathvalidate-3.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil in c:\\programdata\\anaconda3\\lib\\site-packages (from dagshub) (2.9.0.post0)\n",
      "Collecting tenacity~=8.2.2 (from dagshub)\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting boto3 (from dagshub)\n",
      "  Downloading boto3-1.34.161-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from click>=8.0.4->dagshub) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from GitPython>=3.1.29->dagshub) (4.0.7)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx~=0.23.0->dagshub) (2024.7.4)\n",
      "Collecting httpcore<0.17.0,>=0.15.0 (from httpx~=0.23.0->dagshub)\n",
      "  Using cached httpcore-0.16.3-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting rfc3986<2,>=1.3 (from rfc3986[idna2008]<2,>=1.3->httpx~=0.23.0->dagshub)\n",
      "  Using cached rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx~=0.23.0->dagshub) (1.3.0)\n",
      "Collecting commonmark<0.10.0,>=0.9.0 (from rich~=13.1.0->dagshub)\n",
      "  Using cached commonmark-0.9.1-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from rich~=13.1.0->dagshub) (2.17.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from treelib~=1.6.4->dagshub) (1.16.0)\n",
      "Collecting botocore<1.35.0,>=1.34.161 (from boto3->dagshub)\n",
      "  Downloading botocore-1.34.161-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->dagshub) (1.0.1)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->dagshub)\n",
      "  Using cached s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->dagshub)\n",
      "  Using cached marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->dagshub)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from gql[requests]->dagshub) (3.2.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from gql[requests]->dagshub) (1.9.3)\n",
      "Collecting backoff<3.0,>=1.11.1 (from gql[requests]->dagshub)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gql[requests]->dagshub) (4.2.0)\n",
      "Requirement already satisfied: requests<3,>=2.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from gql[requests]->dagshub) (2.32.2)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gql[requests]->dagshub) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->dagshub) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->dagshub) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->dagshub) (2023.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.0->gql[requests]->dagshub) (3.7)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.35.0,>=1.34.161->boto3->dagshub) (2.2.2)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.29->dagshub) (4.0.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore<0.17.0,>=0.15.0->httpx~=0.23.0->dagshub)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->dagshub) (24.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.26->gql[requests]->dagshub) (2.0.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub) (4.11.0)\n",
      "Requirement already satisfied: multidict>=4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.0.4)\n",
      "Using cached dagshub-0.3.34-py3-none-any.whl (236 kB)\n",
      "Using cached dacite-1.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached httpx-0.23.3-py3-none-any.whl (71 kB)\n",
      "Using cached pathvalidate-3.0.0-py3-none-any.whl (21 kB)\n",
      "Using cached rich-13.1.0-py3-none-any.whl (238 kB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached treelib-1.6.4-py3-none-any.whl (18 kB)\n",
      "Downloading boto3-1.34.161-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.2 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/139.2 kB ? eta -:--:--\n",
      "   -------- ------------------------------ 30.7/139.2 kB 640.0 kB/s eta 0:00:01\n",
      "   ----------------- --------------------- 61.4/139.2 kB 656.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 133.1/139.2 kB 871.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- 139.2/139.2 kB 685.8 kB/s eta 0:00:00\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading botocore-1.34.161-py3-none-any.whl (12.5 MB)\n",
      "   ---------------------------------------- 0.0/12.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/12.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/12.5 MB 1.8 MB/s eta 0:00:07\n",
      "    --------------------------------------- 0.2/12.5 MB 1.8 MB/s eta 0:00:07\n",
      "    --------------------------------------- 0.3/12.5 MB 1.8 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.3/12.5 MB 1.5 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.3/12.5 MB 1.4 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.4/12.5 MB 1.4 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.5/12.5 MB 1.4 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.6/12.5 MB 1.4 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.7/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.7/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.8/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.8/12.5 MB 1.4 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.9/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.0/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.2/12.5 MB 1.6 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.2/12.5 MB 1.6 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.3/12.5 MB 1.6 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.4/12.5 MB 1.6 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.5/12.5 MB 1.7 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.6/12.5 MB 1.7 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.7/12.5 MB 1.7 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.8/12.5 MB 1.7 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.9/12.5 MB 1.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.0/12.5 MB 1.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.1/12.5 MB 1.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.1/12.5 MB 1.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.1/12.5 MB 1.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.1/12.5 MB 1.6 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.2/12.5 MB 1.6 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.2/12.5 MB 1.6 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.4/12.5 MB 1.6 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.5/12.5 MB 1.6 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.6/12.5 MB 1.6 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.6/12.5 MB 1.6 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.7/12.5 MB 1.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 2.9/12.5 MB 1.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 2.9/12.5 MB 1.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 3.1/12.5 MB 1.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 3.1/12.5 MB 1.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 3.1/12.5 MB 1.7 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.1/12.5 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.2/12.5 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.2/12.5 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.3/12.5 MB 1.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.4/12.5 MB 1.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.6/12.5 MB 1.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.7/12.5 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.8/12.5 MB 1.7 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.9/12.5 MB 1.7 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 4.1/12.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.2/12.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.3/12.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.3/12.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.3/12.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.3/12.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.3/12.5 MB 1.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.4/12.5 MB 1.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.4/12.5 MB 1.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.5/12.5 MB 1.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.6/12.5 MB 1.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 4.7/12.5 MB 1.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 4.8/12.5 MB 1.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 4.9/12.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.1/12.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.2/12.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.3/12.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.3/12.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.5/12.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.6/12.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.7/12.5 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.9/12.5 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.0/12.5 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.1/12.5 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.2/12.5 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.3/12.5 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.4/12.5 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.6/12.5 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.6/12.5 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.8/12.5 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.9/12.5 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.9/12.5 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.9/12.5 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.9/12.5 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 7.0/12.5 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 7.1/12.5 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.3/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 7.4/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.5/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.6/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.7/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.8/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.9/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.0/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.0/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.0/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.1/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.2/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.3/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.4/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.6/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.7/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.8/12.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.9/12.5 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 9.0/12.5 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.1/12.5 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.2/12.5 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.3/12.5 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.4/12.5 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.5/12.5 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.6/12.5 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.8/12.5 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.9/12.5 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.0/12.5 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.1/12.5 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.2/12.5 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.3/12.5 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.5/12.5 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.6/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.7/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.8/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.8/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.8/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.8/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.8/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.8/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.9/12.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.0/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.1/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.2/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.3/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.4/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.5/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.7/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.7/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.8/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.9/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.1/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.2/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.3/12.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.4/12.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.5/12.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.5/12.5 MB 1.9 MB/s eta 0:00:00\n",
      "Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "Using cached httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
      "Using cached marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "Using cached rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Using cached s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached gql-3.5.0-py2.py3-none-any.whl (74 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: rfc3986, fusepy, commonmark, typing-inspect, treelib, tenacity, rich, pathvalidate, marshmallow, h11, dacite, backoff, httpcore, gql, dataclasses-json, botocore, s3transfer, httpx, boto3, dagshub\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.2.2\n",
      "    Uninstalling tenacity-8.2.2:\n",
      "      Successfully uninstalled tenacity-8.2.2\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 13.3.5\n",
      "    Uninstalling rich-13.3.5:\n",
      "      Successfully uninstalled rich-13.3.5\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.69\n",
      "    Uninstalling botocore-1.34.69:\n",
      "      Successfully uninstalled botocore-1.34.69\n",
      "Successfully installed backoff-2.2.1 boto3-1.34.161 botocore-1.34.161 commonmark-0.9.1 dacite-1.6.0 dagshub-0.3.34 dataclasses-json-0.6.7 fusepy-3.0.1 gql-3.5.0 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 marshmallow-3.21.3 pathvalidate-3.0.0 rfc3986-1.5.0 rich-13.1.0 s3transfer-0.10.2 tenacity-8.2.3 treelib-1.6.4 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.12.3 requires botocore<1.34.70,>=1.34.41, but you have botocore 1.34.161 which is incompatible.\n",
      "streamlit 1.32.0 requires packaging<24,>=16.8, but you have packaging 24.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install dagshub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as rishubjha\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as rishubjha\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"rishubjha/mlops-mini-project\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"rishubjha/mlops-mini-project\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository rishubjha/mlops-mini-project initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository rishubjha/mlops-mini-project initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/15 10:17:38 INFO mlflow.tracking.fluent: Experiment with name 'Logistic Regression Baseline' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/9360bca55bde4fbc933f9f9d4934bbd8', creation_time=1723697259829, experiment_id='0', last_update_time=1723697259829, lifecycle_stage='active', name='Logistic Regression Baseline', tags={}>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dagshub\n",
    "\n",
    "mlflow.set_tracking_uri('https://dagshub.com/rishubjha/mlops-mini-project.mlflow')\n",
    "dagshub.init(repo_owner='rishubjha', repo_name='mlops-mini-project', mlflow=True)\n",
    "\n",
    "mlflow.set_experiment(\"Logistic Regression Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/15 10:20:08 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7739759036144578\n",
      "Precision: 0.7660818713450293\n",
      "Recall: 0.774384236453202\n",
      "F1 Score: 0.7702106810387065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/15 10:20:17 INFO mlflow.tracking._tracking_service.client: 🏃 View run amazing-lamb-237 at: https://dagshub.com/rishubjha/mlops-mini-project.mlflow/#/experiments/0/runs/5eb6c7daa4ba40f6a6b3d76f01520c73.\n",
      "2024/08/15 10:20:17 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://dagshub.com/rishubjha/mlops-mini-project.mlflow/#/experiments/0.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    \n",
    "    # Log preprocessing parameters\n",
    "    mlflow.log_param(\"vectorizer\", \"Bag of Words\")\n",
    "    mlflow.log_param(\"num_features\", 1000)\n",
    "    mlflow.log_param(\"test_size\", 0.2)\n",
    "    \n",
    "    # Model building and training\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Log model parameters\n",
    "    mlflow.log_param(\"model\", \"Logistic Regression\")\n",
    "    \n",
    "    # Model evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Log evaluation metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    # Save and log the notebook\n",
    "    import os\n",
    "    notebook_path = \"exp1_baseline_model.ipynb\"\n",
    "    os.system(f\"jupyter nbconvert --to notebook --execute --inplace {notebook_path}\")\n",
    "    mlflow.log_artifact(notebook_path)\n",
    "    \n",
    "    # Print the results for verification\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
